{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective 1\n",
    "#extract text from FIR images \n",
    "\n",
    "# Generic Libraries\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re,string,unicodedata\n",
    "\n",
    "#Tesseract Library\n",
    "import pytesseract\n",
    "\n",
    "#Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Garbage Collection\n",
    "import gc\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "custom_config = ' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the folder containing the images\n",
    "folder_path = \"FIR_images_v1\"\n",
    "\n",
    "# Get the list of files in the folder\n",
    "image_files = os.listdir(folder_path)\n",
    "\n",
    "# Custom Tesseract configuration if needed\n",
    "custom_config = r'--oem 3 --psm 6'\n",
    "\n",
    "# Iterate over each image file\n",
    "for image_file in image_files:\n",
    "    # Construct the full path to the image\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    \n",
    "    # Load the image using OpenCV\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # Perform OCR using pytesseract\n",
    "    text = pytesseract.image_to_string(img, config=custom_config)\n",
    "\n",
    "    # the output of OCR can be saved in a file in necessary\n",
    "    file = open('output_final.txt','a') # file opened in append mode\n",
    "    file.write(text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#objective 2\n",
    "# code for legal keyword recognization from extracted text \n",
    "\n",
    "import spacy\n",
    "import warnings\n",
    "\n",
    "# Suppress specific warning\n",
    "warnings.filterwarnings(\"ignore\", message=\"Model 'en_blackstone_proto' .*\")\n",
    "\n",
    "try:\n",
    "    # Load English tokenizer, tagger, parser and NER\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except IOError as e:\n",
    "    print(f\"Error loading spaCy model: {e}\")\n",
    "    # Handle the error gracefully, such as loading an alternative model or exiting the program\n",
    "    exit(1)\n",
    "\n",
    "# Read input text file\n",
    "input_file_path = \"output.txt_final\"  # Change this to your input file path\n",
    "try:\n",
    "    with open(input_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read()\n",
    "except UnicodeDecodeError:\n",
    "    print(\"UnicodeDecodeError: Unable to decode the file using UTF-8 encoding.\")\n",
    "    # Handle the decoding error gracefully, such as trying a different encoding or exiting the program\n",
    "    exit(1)\n",
    "\n",
    "# Chunk size for processing\n",
    "chunk_size = 100000  # Adjust as needed based on your text length and available memory\n",
    "\n",
    "# Split the text into chunks\n",
    "chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]\n",
    "\n",
    "# Apply the model to each chunk\n",
    "for chunk in chunks:\n",
    "    doc = nlp(chunk)\n",
    "    \n",
    "    # Output file path for each chunk\n",
    "    output_file_path = \"output_chunk.txt\"  # Change this to your output file path for each chunk\n",
    "    \n",
    "    # Write the entities identified by the model to the output file for each chunk\n",
    "    with open(output_file_path, \"a\", encoding=\"utf-8\") as output_file:\n",
    "        for ent in doc.ents:\n",
    "            output_file.write(f\"{ent.text} - {ent.label_}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
